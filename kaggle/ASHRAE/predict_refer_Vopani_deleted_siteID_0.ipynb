{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASHRAE - Great Energy Predictor III\n",
    "### refer from...\n",
    "- https://www.kaggle.com/rohanrao/ashrae-half-and-half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "%run '../../../utils.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s.imazeki/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "path_data = \"../../input/\"\n",
    "path_train = path_data + \"train.csv\"\n",
    "path_test = path_data + \"test.csv\"\n",
    "path_building = path_data + \"building_metadata.csv\"\n",
    "path_weather_train = path_data + \"weather_train.csv\"\n",
    "path_weather_test = path_data + \"weather_test.csv\"\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "myfavouritenumber = 0\n",
    "seed = myfavouritenumber\n",
    "random.seed(seed)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path_train)\n",
    "\n",
    "building = pd.read_csv(path_building)\n",
    "le = LabelEncoder()\n",
    "building.primary_use = le.fit_transform(building.primary_use)\n",
    "\n",
    "weather_train = pd.read_csv(path_weather_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 289.19 Mb (53.1% reduction)\n",
      "Mem. usage decreased to  0.02 Mb (74.9% reduction)\n",
      "Mem. usage decreased to  3.07 Mb (68.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "building = reduce_mem_usage(building)\n",
    "weather_train = reduce_mem_usage(weather_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete site_id: 0\n",
    "df_train = df_train.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['meter_reading_log1p'] = np.log1p(df_train['meter_reading'])\n",
    "df_group = df_train.groupby('building_id')['meter_reading_log1p']\n",
    "building_mean = df_group.mean().astype(np.float16)\n",
    "building_median = df_group.median().astype(np.float16)\n",
    "building_min = df_group.min().astype(np.float16)\n",
    "building_max = df_group.max().astype(np.float16)\n",
    "building_std = df_group.std().astype(np.float16)\n",
    "\n",
    "df_train['building_mean'] = df_train['building_id'].map(building_mean)\n",
    "df_train['building_median'] = df_train['building_id'].map(building_median)\n",
    "df_train['building_min'] = df_train['building_id'].map(building_min)\n",
    "df_train['building_max'] = df_train['building_id'].map(building_max)\n",
    "df_train['building_std'] = df_train['building_id'].map(building_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, building_data, weather_data, test=False):\n",
    "    \"\"\"\n",
    "    Preparing final dataset with all features.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X.merge(building_data, on=\"building_id\", how=\"left\")\n",
    "    X = X.merge(weather_data, on=[\"site_id\", \"timestamp\"], how=\"left\")\n",
    "    \n",
    "    X.timestamp = pd.to_datetime(X.timestamp, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    X.square_feet = np.log1p(X.square_feet)\n",
    "    \n",
    "    if not test:\n",
    "        X.sort_values(\"timestamp\", inplace=True)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "                \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "                \"2017-01-01\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "                \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "                \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "                \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "                \"2019-01-01\"]\n",
    "    \n",
    "    X[\"hour\"] = X.timestamp.dt.hour\n",
    "    X[\"weekday\"] = X.timestamp.dt.weekday\n",
    "    X[\"is_holiday\"] = (X.timestamp.dt.date.astype(\"str\").isin(holidays)).astype(int)\n",
    "    \n",
    "    for window in [3, 72]:\n",
    "        group_df = X.groupby('site_id')\n",
    "        cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n",
    "        rolled = group_df[cols].rolling(window=window, min_periods=0)\n",
    "        lag_mean = rolled.mean().reset_index().astype(np.float16)\n",
    "        lag_max = rolled.max().reset_index().astype(np.float16)\n",
    "        lag_min = rolled.min().reset_index().astype(np.float16)\n",
    "        lag_std = rolled.std().reset_index().astype(np.float16)\n",
    "        for col in cols:\n",
    "            X[f'{col}_mean_lag{window}'] = lag_mean[col]\n",
    "            X[f'{col}_max_lag{window}'] = lag_max[col]\n",
    "            X[f'{col}_min_lag{window}'] = lag_min[col]\n",
    "            X[f'{col}_std_lag{window}'] = lag_std[col]\n",
    "    \n",
    "    drop_features = [\"timestamp\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\"]\n",
    "\n",
    "    X.drop(drop_features, axis=1, inplace=True)\n",
    "\n",
    "    if test:\n",
    "        row_ids = X.row_id\n",
    "        X.drop(\"row_id\", axis=1, inplace=True)\n",
    "        return X, row_ids\n",
    "    else:\n",
    "        y = np.log1p(X.meter_reading)\n",
    "        X.drop(\"meter_reading\", axis=1, inplace=True)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = prepare_data(df_train, building, weather_train)\n",
    "\n",
    "del df_train, weather_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-fold LightGBM Model split half-and-half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with first half and validating on second half:\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 0.848882\tvalid_1's rmse: 1.14964\n",
      "[400]\ttraining's rmse: 0.791538\tvalid_1's rmse: 1.14681\n",
      "Early stopping, best iteration is:\n",
      "[353]\ttraining's rmse: 0.80131\tvalid_1's rmse: 1.14626\n",
      "Building model with second half and validating on first half:\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 0.843844\tvalid_1's rmse: 1.15236\n",
      "[400]\ttraining's rmse: 0.784397\tvalid_1's rmse: 1.14981\n",
      "Early stopping, best iteration is:\n",
      "[352]\ttraining's rmse: 0.794341\tvalid_1's rmse: 1.14925\n",
      "CPU times: user 2h 2min 6s, sys: 2min 51s, total: 2h 4min 58s\n",
      "Wall time: 44min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_half_1 = X_train[:int(X_train.shape[0] / 2)]\n",
    "X_half_2 = X_train[int(X_train.shape[0] / 2):]\n",
    "\n",
    "y_half_1 = y_train[:int(X_train.shape[0] / 2)]\n",
    "y_half_2 = y_train[int(X_train.shape[0] / 2):]\n",
    "\n",
    "categorical_features = [\"building_id\", \"site_id\", \"meter\", \"primary_use\", \"hour\", \"weekday\"]\n",
    "\n",
    "d_half_1 = lgb.Dataset(X_half_1, label=y_half_1, categorical_feature=categorical_features, free_raw_data=False)\n",
    "d_half_2 = lgb.Dataset(X_half_2, label=y_half_2, categorical_feature=categorical_features, free_raw_data=False)\n",
    "\n",
    "watchlist_1 = [d_half_1, d_half_2]\n",
    "watchlist_2 = [d_half_2, d_half_1]\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"num_leaves\": 40,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.85,\n",
    "    \"reg_lambda\": 2,\n",
    "    \"metric\": \"rmse\"\n",
    "}\n",
    "\n",
    "print(\"Building model with first half and validating on second half:\")\n",
    "model_half_1 = lgb.train(params, train_set=d_half_1, num_boost_round=1000, valid_sets=watchlist_1, verbose_eval=200, early_stopping_rounds=200)\n",
    "\n",
    "print(\"Building model with second half and validating on first half:\")\n",
    "model_half_2 = lgb.train(params, train_set=d_half_2, num_boost_round=1000, valid_sets=watchlist_2, verbose_eval=200, early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_half_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-069dc9191407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermed_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_half_1_2.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_half_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermed_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_half_2_2.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_half_1' is not defined"
     ]
    }
   ],
   "source": [
    "intermed_path = './tmp/Vopani/'\n",
    "\n",
    "with open(os.path.join(intermed_path, 'model_half_1_2.pickle'), mode='wb') as fp:\n",
    "    pickle.dump(model_half_1, fp)\n",
    "    \n",
    "with open(os.path.join(intermed_path, 'model_half_2_2.pickle'), mode='wb') as fp:\n",
    "    pickle.dump(model_half_2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermed_path = './tmp/Vopani/'\n",
    "\n",
    "model_half_1 = pd.read_pickle(os.path.join(intermed_path, 'model_half_1.pickle'))\n",
    "model_half_2 = pd.read_pickle(os.path.join(intermed_path, 'model_half_2.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-76ba9d652726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_fimp_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_fimp_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feature\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_fimp_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"importance\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_half_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_fimp_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"half\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "df_fimp_1 = pd.DataFrame()\n",
    "df_fimp_1[\"feature\"] = X_train.columns.values\n",
    "df_fimp_1[\"importance\"] = model_half_1.feature_importance()\n",
    "df_fimp_1[\"half\"] = 1\n",
    "\n",
    "df_fimp_2 = pd.DataFrame()\n",
    "df_fimp_2[\"feature\"] = X_train.columns.values\n",
    "df_fimp_2[\"importance\"] = model_half_2.feature_importance()\n",
    "df_fimp_2[\"half\"] = 2\n",
    "\n",
    "df_fimp = pd.concat([df_fimp_1, df_fimp_2], axis=0)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=df_fimp.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title(\"LightGBM Feature Importance\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 596.49 Mb (53.1% reduction)\n",
      "Mem. usage decreased to  6.08 Mb (68.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(path_test)\n",
    "weather_test = pd.read_csv(path_weather_test)\n",
    "\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "weather_test = reduce_mem_usage(weather_test)\n",
    "\n",
    "X_test, row_ids = prepare_data(df_test, building, weather_test, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['building_mean'] = df_test['building_id'].map(building_mean)\n",
    "df_test['building_median'] = df_test['building_id'].map(building_median)\n",
    "df_test['building_min'] = df_test['building_id'].map(building_min)\n",
    "df_test['building_max'] = df_test['building_id'].map(building_max)\n",
    "df_test['building_std'] = df_test['building_id'].map(building_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 49min 45s, sys: 7min 47s, total: 2h 57min 32s\n",
      "Wall time: 1h 6min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred = np.expm1(model_half_1.predict(X_test, num_iteration=model_half_1.best_iteration)) / 2\n",
    "\n",
    "del model_half_1\n",
    "gc.collect()\n",
    "\n",
    "pred += np.expm1(model_half_2.predict(X_test, num_iteration=model_half_2.best_iteration)) / 2\n",
    "\n",
    "del model_half_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 12s, sys: 6.59 s, total: 3min 19s\n",
      "Wall time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "submission = pd.DataFrame({\"row_id\": row_ids, \"meter_reading\": np.clip(pred, 0, a_max=None)})\n",
    "submission.loc[submission['meter_reading'] < 0, 'meter_reading'] = 0\n",
    "submission.to_csv('../../output/' + 'sub_' + str(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "i = 0\n",
    "res = []\n",
    "step_size = 50000\n",
    "for j in tqdm(range(int(np.ceil(X_test.shape[0] / 50000)))):\n",
    "    res.append(np.expm1(sum([model.predict(X_test.iloc[i:i + step_size]) for model in [model_half_1, model_half_2]]) / 2))\n",
    "    i += step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
